---
title: 'Simulating User Conversations'
description: "Learn how to use Galtea's Conversation Simulator to test your AI with a synthetic user."
icon: "user-group"
---

import AgentCallables from '/snippets/agent-callables.mdx';

Galtea's [Conversation Simulator](/concepts/product/test/case/conversation-simulator) allows you to test your conversational AI products by simulating realistic user interactions. This guide walks you through integrating your agent and running simulations.

## Agent Integration Options

<AgentCallables />

## Conversation Simulation Workflow

<Steps>
  <Step title="1. Implement Your Agent">
    Define an agent function with one of the supported signatures above.
  </Step>
  <Step title="2. Prepare Scenario Data">
    Create a CSV file with scenario data. Each row is a test case describing the user goal, persona, and input (the first user message).
  </Step>
  <Step title="3. Create a Test and Sessions">
    Upload your scenario CSV to create a test. The platform generates a session for each scenario.
  </Step>
  <Step title="4. Run the Simulator with Your Agent">
    Use <code>SimulatorService.simulate()</code> to execute the conversation between your agent and the synthetic user, for each session.
  </Step>
  <Step title="5. Evaluate the Results">
    After simulation, analyze results and optionally trigger evaluations via <code>evaluations.create()</code>.
  </Step>
</Steps>

---

### Step-by-Step Guide

#### 1. Create a Test and Sessions

In order to run simulations of conversations we need to have different behavior test cases, each with their user personas. The easiest way to create these is to use the behavior test creation feature of the platform.

```python
# Create a test suite using the behavior test options
# This can be done via the Dashboard or programmatically as shown here
test = galtea_client.tests.create(
    product_id=product.id,
    name=test_name,
    type="BEHAVIOR",
    # This time we provide a path to a CSV file with behavior tests, but you can also have Galtea generate them if you do not provide a CSV file
    test_file_path="path/to/behavior_test.csv",
)

# Get your test cases
# If Galtea is generating the test for you, it might take a few moments to be ready
test_cases = galtea_client.test_cases.list(test_id=test.id)
```

After some time you should see the resulting generated test cases in your dashboard. Another option is to upload a csv file with the necessary fields, for more information on the fields see [Behavior Tests](/concepts/product/test/scenario-tests) for more details.

#### 2. Run the Conversation Simulator

For each test case/session, use the simulator to run the full simulation with your agent function:

```python
# Define your agent function (see Agent Integration Options for all signatures)
def my_agent(user_message: str) -> str:
    return f"Response to: {user_message}"


# Run simulations with your agent function
for test_case in test_cases:
    session = galtea_client.sessions.create(
        version_id=version.id, test_case_id=test_case.id
    )

    result = galtea_client.simulator.simulate(
        session_id=session.id, agent=my_agent, max_turns=10
    )

    # Analyze results
    print(f"Scenario: {test_case.scenario}")
    print(f"Completed {result.total_turns} turns")
    print(f"Success: {result.finished}")
    if result.stopping_reason:
        print(f"Ended because: {result.stopping_reason}")
```

<Info>
    You can optionally use the `@trace` decorator to capture internal operations during simulation. Traces are automatically collected and saved per turn.
    See the [Tracing Agent Operations](/sdk/tutorials/tracing-agent-operations) guide for more details on using the `@trace` decorator.
</Info>

#### 3. Evaluate the Session

```python
    # After each simulation, you can create an evaluation
    evaluations = galtea_client.evaluations.create(
        session_id=session.id,
        metrics=[{"name": "Role Adherence"}],  # Replace with your metrics
    )
    for evaluation in evaluations:
        print(f"Evaluation created: {evaluation.id}")
```

---

## Advanced Usage: RAG Agents with Retrieval Context

For Retrieval-Augmented Generation (RAG) agents, you can return the context that was retrieved and used to generate the response. This context will be logged with the inference result, enabling powerful evaluations with metrics like `Faithfulness` and `Contextual Relevancy`.

```python
def my_rag_agent(input_data: galtea.AgentInput) -> galtea.AgentResponse:
    user_message = input_data.last_user_message_str()

    # Your RAG logic to retrieve context and generate a response
    retrieved_docs = vector_store.search(user_message)
    response_content = llm.generate(prompt=user_message, context=retrieved_docs)

    return galtea.AgentResponse(
        content=response_content,
        retrieval_context=retrieved_docs,
        metadata={"docs_retrieved": len(retrieved_docs)},
    )
```

The `retrieval_context` field is optional and can contain:
- Retrieved document snippets or full documents
- Formatted context strings
- JSON-serializable data structures

By providing retrieval context, you enable Galtea to evaluate the faithfulness of your model's responses relative to the retrieved information, which is crucial for assessing RAG system quality.

---

By using agent functions and the simulation method, you can quickly evaluate your conversational AI models in realistic, repeatable conditions, leveraging Galtea's powerful simulation and analytics tools.
