---
title: 'Evaluate with Custom Metrics'
description: "Learn how to run evaluations with your own custom, self-hosted metrics."
icon: "vial-circle-check"
iconType: "solid"
---

Galtea allows you to define and use your own custom metrics for evaluations. This is particularly useful for:
-   **Deterministic Metrics**: When you have custom, rule-based logic to score outputs (e.g., checking for specific keywords, validating JSON structure).
-   **Integrating External Models**: When you use your own models for evaluation and want to store their scores in Galtea.

## Recommended Approach: MetricInput Dictionary

The recommended way to specify metrics in SDK v3.0 is using the `MetricInput` dictionary format. For self-hosted metrics, you have two equally valid options for providing scores:

### Option 1: Pre-Compute the Score

If you want to calculate the score yourself before creating the evaluation, you can provide the score directly as a float:

```python
# Your product's response
actual_output = "This response contains the correct keyword."


# Define your custom scoring logic
def contains_keyword(text: str, keyword: str) -> float:
    """Returns 1.0 if the keyword is in text (case-insensitive), 0.0 otherwise."""
    return 1.0 if keyword.lower() in text.lower() else 0.0


# Compute the score
custom_score = contains_keyword(actual_output, "correct")

# Create the metric if it doesn't exist yet
CUSTOM_METRIC_NAME = "contains-correct"
galtea.metrics.create(
    name=CUSTOM_METRIC_NAME,
    source="self_hosted",
    description='Checks for the presence of the keyword "correct" in the output',
    test_type="QUALITY",
)

# Run evaluation with your pre-computed score
galtea.evaluations.create_single_turn(
    version_id=version_id,
    test_case_id=test_case_id,
    actual_output=actual_output,
    metrics=[
        {"name": "Role Adherence"},  # Standard Galtea metric
        {
            "name": CUSTOM_METRIC_NAME,
            "score": custom_score,
        },  # Self-hosted with pre-computed score
    ],
)
```

### Option 2: Use CustomScoreEvaluationMetric Class

If you prefer to encapsulate your scoring logic in a class that will be executed dynamically, you can use the `CustomScoreEvaluationMetric` class within the MetricInput dictionary:

```python
# Define your custom metric class
class ContainsKeyword(CustomScoreEvaluationMetric):
    def __init__(self, keyword: str):
        self.keyword = keyword.lower()
        # Initialize with the metric name or ID
        super().__init__(name=f"contains-{self.keyword}")

    def measure(self, *args, actual_output: str | None = None, **kwargs) -> float:
        """
        Returns 1.0 if the keyword is in actual_output (case-insensitive), 0.0 otherwise.
        All other args/kwargs are accepted but ignored.
        """
        if actual_output is None:
            return 0.0
        return 1.0 if self.keyword in actual_output.lower() else 0.0


# Instantiate your metric
accuracy_metric = ContainsKeyword(keyword="relevant")

# Create the metric in the platform if it doesn't exist yet
galtea.metrics.create(
    name=accuracy_metric.name,
    source="self_hosted",
    description='Checks for the presence of the keyword "relevant" in the output',
    test_type="QUALITY",
)

# Your product's response
actual_output_2 = "This response is relevant and helpful."

# Run evaluation with your custom metric class
# Important: Do NOT provide 'id' or 'name' in the dict when using CustomScoreEvaluationMetric
galtea.evaluations.create_single_turn(
    version_id=version_id,
    test_case_id=test_case_id,
    actual_output=actual_output_2,
    metrics=[
        {"name": "Role Adherence"},  # Standard Galtea metric
        {"score": accuracy_metric},  # Self-hosted with dynamic scoring
    ],
)
```

<Note>
  When using `CustomScoreEvaluationMetric` as the `score` field in a MetricInput dictionary, do NOT provide `id` or `name` in the dictionary itself. The metric identifier must be specified when initializing the CustomScoreEvaluationMetric instance (e.g., `CustomScoreEvaluationMetric(name="my-metric")`).
</Note>

## Choosing Between Options

Both approaches are equally valid and current. Choose based on your preference:

- **Use Option 1 (Pre-Computed Score)** if:
  - You prefer a simpler, more declarative style
  - Your scoring logic is straightforward and doesn't require encapsulation
  - You want to separate score calculation from the evaluation submission

- **Use Option 2 (CustomScoreEvaluationMetric Class)** if:
  - You prefer object-oriented design
  - Your scoring logic is complex and benefits from encapsulation
  - You want the SDK to handle score calculation automatically
  - You need to reuse the same metric logic across multiple evaluations
